{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadPositionAwareSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadPositionAwareSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        # Learnable positional relationships per head\n",
    "        self.aij_Q = nn.Parameter(torch.randn(n_heads, 9, 9, self.head_dim))\n",
    "        self.aij_K = nn.Parameter(torch.randn(n_heads, 9, 9, self.head_dim))\n",
    "        self.aij_V = nn.Parameter(torch.randn(n_heads, 9, 9, self.head_dim))\n",
    "\n",
    "        # Linear projections for multi-head attention\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_tokens, _ = x.shape\n",
    "        assert n_tokens == 9, \"This implementation expects exactly 9 tokens\"\n",
    "\n",
    "        # Compute projections and reshape for heads\n",
    "        Q = self.W_Q(x).view(batch_size, n_tokens, self.n_heads, self.head_dim)\n",
    "        K = self.W_K(x).view(batch_size, n_tokens, self.n_heads, self.head_dim)\n",
    "        V = self.W_V(x).view(batch_size, n_tokens, self.n_heads, self.head_dim)\n",
    "\n",
    "        # Initialize attention logits and output\n",
    "        attention_logits = torch.zeros(batch_size, self.n_heads, n_tokens, n_tokens, device=x.device)\n",
    "        attention_output = torch.zeros(batch_size, self.n_heads, n_tokens, self.head_dim, device=x.device)\n",
    "\n",
    "        # Loop through all token pairs and heads\n",
    "        for h in range(self.n_heads):\n",
    "            for i in range(n_tokens):\n",
    "                for j in range(n_tokens):\n",
    "                    # Calculate adjusted query and key for the current head\n",
    "                    qi = Q[:, i, h, :] + self.aij_Q[h, i, j]\n",
    "                    kj = K[:, j, h, :] + self.aij_K[h, i, j]\n",
    "\n",
    "                    # Compute attention logit\n",
    "                    attention_logits[:, h, i, j] = (qi * kj).sum(dim=-1) / math.sqrt(self.head_dim)\n",
    "\n",
    "                    # Update attention output\n",
    "                    vj = V[:, j, h, :] + self.aij_V[h, i, j]\n",
    "                    attention_output[:, h, i, :] += attention_logits[:, h, i, j].unsqueeze(-1) * vj\n",
    "\n",
    "        # Normalize attention logits across tokens\n",
    "        attention_weights = torch.softmax(attention_logits, dim=-1)\n",
    "\n",
    "        # Finalize output, rescaling weights and concatenating the head outputs\n",
    "        outputs = []\n",
    "        for h in range(self.n_heads):\n",
    "            head_output = torch.matmul(attention_weights[:, h], attention_output[:, h])\n",
    "            outputs.append(head_output)\n",
    "        out = torch.cat(outputs, dim=-1)\n",
    "\n",
    "        # Concatenate head outputs and project back to original dimensionality\n",
    "        out = out.view(batch_size, n_tokens, self.d_model)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 9, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example usage for tic-tac-toe\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "\n",
    "self_attention = MultiHeadPositionAwareSelfAttention(d_model, n_heads)\n",
    "\n",
    "# Input tensor: batch_size x n_tokens x d_model\n",
    "x = torch.randn(32, 9, d_model)  # Batch of 32 games with 9 tokens each\n",
    "output = self_attention(x)\n",
    "print(output.shape)  # Should be (32, 9, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadPositionAwareSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadPositionAwareSelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        # Learnable positional relationships per head\n",
    "        self.aij_Q = nn.Parameter(torch.randn(n_heads, 9, 9, self.head_dim))  # n_heads x 9 x 9 x head_dim\n",
    "        self.aij_K = nn.Parameter(torch.randn(n_heads, 9, 9, self.head_dim))\n",
    "        self.aij_V = nn.Parameter(torch.randn(n_heads, 9, 9, self.head_dim))\n",
    "\n",
    "        # Linear projections for multi-head attention\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_tokens, _ = x.shape\n",
    "        assert n_tokens == 9, \"This implementation expects exactly 9 tokens\"\n",
    "\n",
    "        # Compute projections and reshape for heads\n",
    "        Q = self.W_Q(x).view(batch_size, n_tokens, self.n_heads, self.head_dim)  # (B, T, H, D_h)\n",
    "        K = self.W_K(x).view(batch_size, n_tokens, self.n_heads, self.head_dim)  # (B, T, H, D_h)\n",
    "        V = self.W_V(x).view(batch_size, n_tokens, self.n_heads, self.head_dim)  # (B, T, H, D_h)\n",
    "\n",
    "        # Rearrange dimensions for broadcasting\n",
    "        Q = Q.permute(2, 0, 1, 3).unsqueeze(3)  # (H, B, T, 1, D_h)\n",
    "        K = K.permute(2, 0, 1, 3).unsqueeze(2)  # (H, B, 1, T, D_h)\n",
    "        V = V.permute(2, 0, 1, 3) # (H, B, T, D_h)\n",
    "\n",
    "        # Positional adjustments (broadcasted across batches and tokens)\n",
    "        Q = Q + self.aij_Q.unsqueeze(1)  # (H, B, T, T, D_h)\n",
    "        K = K + self.aij_K.unsqueeze(1)  # (H, B, T, T, D_h)\n",
    "\n",
    "        # Compute attention logits\n",
    "        attention_logits = torch.einsum(\"hbijk,hbijk->hbik\", Q, K) / math.sqrt(self.head_dim)  # (H, B, T, T)\n",
    "\n",
    "        # Normalize logits to obtain weights\n",
    "        attention_weights = torch.softmax(attention_logits, dim=-1)  # (H, B, T, T)\n",
    "\n",
    "        # Compute attention output\n",
    "        V = V.unsqueeze(2)  # (H, B, 1, T, D_h)\n",
    "        aij_V_expanded = self.aij_V.unsqueeze(1)  # (H, 1, T, T, D_h)\n",
    "        V_adjusted = V + aij_V_expanded # (H, B, T, T, D_h)\n",
    "        attention_output = torch.einsum(\"hbij,hbijk->hbik\", attention_weights, V_adjusted)  # (H, B, T, D_h)\n",
    "\n",
    "        # Reshape and combine heads\n",
    "        attention_output = attention_output.permute(1, 2, 0, 3).reshape(batch_size, n_tokens, self.d_model)  # (B, T, D)\n",
    "\n",
    "        # Project back to original dimensionality\n",
    "        out = self.fc_out(attention_output)  # (B, T, D)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript j has size 9 for operand 1 which does not broadcast with previously seen size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Input tensor: batch_size x n_tokens x d_model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m9\u001b[39m, d_model)  \u001b[38;5;66;03m# Batch of 32 games with 9 tokens each\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\tyler\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tyler\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mMultiHeadPositionAwareSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m aij_V_expanded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maij_V\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (H, 1, T, T, D_h)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m V_adjusted \u001b[38;5;241m=\u001b[39m V \u001b[38;5;241m+\u001b[39m aij_V_expanded \u001b[38;5;66;03m# (H, B, T, T, D_h)\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhbij,hbijk->hbik\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_adjusted\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (H, B, T, D_h)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Reshape and combine heads\u001b[39;00m\n\u001b[0;32m     54\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, n_tokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)  \u001b[38;5;66;03m# (B, T, D)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tyler\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:385\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    387\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript j has size 9 for operand 1 which does not broadcast with previously seen size 8"
     ]
    }
   ],
   "source": [
    "# Example usage for tic-tac-toe\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "\n",
    "self_attention = MultiHeadPositionAwareSelfAttention(d_model, n_heads)\n",
    "\n",
    "# Input tensor: batch_size x n_tokens x d_model\n",
    "x = torch.randn(32, 9, d_model)  # Batch of 32 games with 9 tokens each\n",
    "output = self_attention(x)\n",
    "print(output.shape)  # Should be (32, 9, d_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
